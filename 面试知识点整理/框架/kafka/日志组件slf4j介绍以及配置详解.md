# kafka官方文档学习

# https://kafka.apachecn.org/documentation.html#producerapi

### 与大多数消息系统相比，Kafka拥有更好的吞吐量、内置分区、具有复制和容错的功能，这使它成为一个非常理想的大型消息处理应用。

#### tropics和日志

对于每一个tropic，kafka集群都会维持一个分区日志。

每个分区都是一个有序且顺序不可变的记录集，并且不断追加到结构化的commit log文件中。

分区中的每个记录都会分配一个id号来标识顺序，称之为offset

Kafka 集群保留所有发布的记录—无论他们是否已被消费—并通过一个可配置的参数——保留期限来控制.

在每一个消费者中唯一保存的元数据是offset（偏移量）即消费在log中的位置.

日志中的 partition（分区）有以下几个用途。

- 第一，当日志大小超过了单台服务器的限制，允许日志进行扩展。每个单独的分区都必须受限于主机的文件限制，不过一个主题可能有多个分区，因此可以处理无限量的数据。
- 第二，可以作为并行的单元集

#### 分布式

每个分区都有一台 server 作为 “leader”，零台或者多台server作为 follwers 

leader server 处理一切对 partition （分区）的读写请求，而follwers只需被动的同步leader上的数据。

当leader宕机了，followers 中的一台服务器会自动成为新的 leader。

每台 server 都会成为某些分区的 leader 和某些分区的 follower，因此集群的负载是平衡的。    

#### 生产者

生产者可以将数据发布到所选择的topic（主题）中。

生产者负责将记录分配到topic的哪一个 partition（分区）中。

可以使用循环的方式来简单地实现负载均衡，也可以根据某些语义分区函数(例如：记录中的key)来完成。

#### 消费者

消费者使用一个 *消费组* 名称来进行标识，发布到topic中的每条记录被分配给订阅消费组中的一个消费者实例.

消费者实例可以分布在多个进程中或者多个机器上。    

如果所有的消费者实例在同一消费组中，消息记录会负载平衡到每一个消费者实例.

如果所有的消费者实例在不同的消费组中，每条消息记录会广播到所有的消费者进程.    

在Kafka中实现消费的方式是将日志中的分区划分到每一个消费者实例上，以便在任何时间，每个实例都是分区唯一的消费者。

​	维护消费组中的消费关系由Kafka协议动态处理。如果新的实例加入组，他们将从组中其他成员处接管一些 partition 分区;如果一个实例消失，拥有的分区将被分发到剩余的实例。    

​	Kafka 只保证分区内的记录是有序的，而不保证主题中不同分区的顺序。每个 partition 分区按照key值排序足以满足大多数应用程序的需求。但如果你需要总记录在所有记录的上面，可使用仅有一个分区的主题来实现，这意味着每个消费者组只有一个消费者进程。    

#### 保证

​	生产者发送到特定topic partition 的消息将按照发送的顺序处理。也就是说，如果记录M1和记录M2由相同的生产者发送，并先发送M1记录，那么M1的偏移比M2小，并在日志中较早出现

​	一个消费者实例按照日志中的顺序查看记录.

#### kafka作为消息系统

消费组在Kafka有两层概念。在队列中，消费组允许你将处理过程分发给一系列进程(消费组中的成员)。在发布订阅中，Kafka允许你将消息广播给多个消费组。

Kafka的优势在于每个topic都有以下特性—可以扩展处理并且允许多订阅者模式—不需要只选择其中一个.    

Kafka能够为一个消费者池提供顺序保证和负载平衡，是通过将topic中的partition分配给消费者组中的消费者来实现的，以便每个分区由消费组中的一个消费者消耗。

通过这样，我们能够确保消费者是该分区的唯一读者，并按顺序消费数据。        

众多分区保证了多个消费者实例间的负载均衡。

消费者组中的消费者实例个数不能超过分区的数量。    

#### kafka作为存储系统

数据写入Kafka后被写到磁盘，并且进行备份以便容错。直到完全备份，Kafka才让生产者认为完成写入，即使写入失败Kafka也会确保继续写入

Kafka使用磁盘结构，具有很好的扩展性—50kb和50TB的数据在server上表现一致。

可以存储大量数据，并且可通过客户端控制它读取数据的位置，您可认为Kafka是一种高性能、低延迟、具备日志存储、备份和传播功能的分布式文件系统。

#### kafka作为流处理

简单的数据处理可以直接用生产者和消费者的API。对于复杂的数据变换，Kafka提供了Streams API。        Stream API 允许应用做一些复杂的处理，比如将流数据聚合或者join。    

有助于解决以下这种应用程序所面临的问题：处理无序数据，当消费端代码变更后重新处理输入，执行有状态计算等。    

Streams API建立在Kafka的核心之上：它使用Producer和Consumer API作为输入，使用Kafka进行有状态的存储，        并在流处理器实例之间使用相同的消费组机制来实现容错。  

####  持久化

[顺序磁盘访问在某些情况下比随机内存访问还要快](http://deliveryimages.acm.org/10.1145/1570000/1563874/jacobs3.jpg)！



## 问题

- Kafka的性能和数据大小无关？
- 每个单独的分区都必须受限于主机的文件限制，不过一个主题可能有多个分区，因此可以处理无限量的数据。
- 消费组是怎么划分的？
- 在Kafka中实现消费的方式是将日志中的分区划分到每一个消费者实例上，以便在任何时间，每个实例都是分区唯一的消费者。维护消费组中的消费关系由Kafka协议动态处理。如果新的实例加入组，他们将从组中其他成员处接管一些 partition 分区;如果一个实例消失，拥有的分区将被分发到剩余的实例？